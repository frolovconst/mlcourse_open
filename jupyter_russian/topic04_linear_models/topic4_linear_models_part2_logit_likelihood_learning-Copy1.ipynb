{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Юрий Кашницкий\n",
    "    \n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFlCAYAAADComBzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f08828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
